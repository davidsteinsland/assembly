#include "simd.h"

/*
  assuming that a and b is aligned to 16 bytes (!!!!)

  we are using uint16_t because then we can fit eight of them
  in each 128-bit XMM register.
*/
void transpose_8x8_int16(uint16_t src[8 * 8], uint16_t out[8 * 8]) {
  /*
    consider input:

    row +----+----+----+----+----+----+----+----+
     0  |  1 |  2 |  3 |  4 |  5 |  6 |  7 |  8 |
        +----+----+----+----+----+----+----+----+
     1  |  9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 |
        +----+----+----+----+----+----+----+----+
     2  | 17 | 18 | 19 | 20 | 21 | 22 | 23 | 24 |
        +----+----+----+----+----+----+----+----+
     3  | 25 | 26 | 27 | 28 | 29 | 30 | 31 | 32 |
        +----+----+----+----+----+----+----+----+
     4  | 33 | 34 | 35 | 36 | 37 | 38 | 39 | 40 |
        +----+----+----+----+----+----+----+----+
     5  | 41 | 42 | 43 | 44 | 45 | 46 | 47 | 48 |
        +----+----+----+----+----+----+----+----+
     6  | 49 | 50 | 51 | 52 | 53 | 54 | 55 | 56 |
        +----+----+----+----+----+----+----+----+
     7  | 57 | 58 | 59 | 60 | 61 | 62 | 63 | 64 |
        +----+----+----+----+----+----+----+----+
   */
  __m128i* in = (__m128i*)src;

  __m128i row0, row1, row2, row3, row4, row5, row6, row7;

  row0 = in[0];
  row1 = in[1];
  row2 = in[2];
  row3 = in[3];
  row4 = in[4];
  row5 = in[5];
  row6 = in[6];
  row7 = in[7];

  __m128i a, b, c, d;
  __m128i p, q;

  /*
    +----+----+----+----+----+----+----+----+
    |  1 | 17 |  2 | 18 |  3 | 19 |  4 | 20 |
    +----+----+----+----+----+----+----+----+
  */
  a = _mm_unpacklo_epi16(row0, row2);
  /*
    +----+----+----+----+----+----+----+----+
    |  9 | 25 | 10 | 26 | 11 | 27 | 12 | 28 |
    +----+----+----+----+----+----+----+----+
   */
  b = _mm_unpacklo_epi16(row1, row3);
  /*
    +----+----+----+----+----+----+----+----+
    |  1 |  9 | 17 | 25 |  2 | 10 | 18 | 26 |
    +----+----+----+----+----+----+----+----+
   */
  p = _mm_unpacklo_epi16(a, b);

  /*
    +----+----+----+----+----+----+----+----+
    | 33 | 49 | 34 | 50 | 35 | 51 | 36 | 52 |
    +----+----+----+----+----+----+----+----+
   */
  c = _mm_unpacklo_epi16(row4, row6);
  /*
    +----+----+----+----+----+----+----+----+
    | 41 | 57 | 42 | 58 | 43 | 59 | 44 | 60 |
    +----+----+----+----+----+----+----+----+
   */
  d = _mm_unpacklo_epi16(row5, row7);
  /*
    +----+----+----+----+----+----+----+----+
    | 33 | 41 | 49 | 57 | 34 | 42 | 50 | 58 |
    +----+----+----+----+----+----+----+----+
   */
  q = _mm_unpacklo_epi16(c, d);

  /*
    +----+----+----+----+----+----+----+----+
    |  1 |  9 | 17 | 25 | 33 | 41 | 49 | 57 |
    +----+----+----+----+----+----+----+----+
   */
  _mm_store_si128(&((__m128i*)out)[0], _mm_unpacklo_epi64(p, q));
  /*
    +----+----+----+----+----+----+----+----+
    |  2 | 10 | 18 | 26 | 34 | 42 | 50 | 58 |
    +----+----+----+----+----+----+----+----+
   */
  _mm_store_si128(&((__m128i*)out)[1], _mm_unpackhi_epi64(p, q));

  /*
    +----+----+----+----+----+----+----+----+
    |  3 | 11 | 19 | 27 |  4 | 12 | 20 | 28 |
    +----+----+----+----+----+----+----+----+
   */
  p = _mm_unpackhi_epi16(a, b);
  /*
    +----+----+----+----+----+----+----+----+
    | 35 | 43 | 51 | 59 | 36 | 44 | 52 | 60 |
    +----+----+----+----+----+----+----+----+
   */
  q = _mm_unpackhi_epi16(c, d);

  /*
    +----+----+----+----+----+----+----+----+
    |  3 | 11 | 19 | 27 | 35 | 43 | 51 | 59 |
    +----+----+----+----+----+----+----+----+
   */
  _mm_store_si128(&((__m128i*)out)[2], _mm_unpacklo_epi64(p, q));
  /*
    +----+----+----+----+----+----+----+----+
    |  4 | 12 | 20 | 28 | 36 | 44 | 52 | 60 |
    +----+----+----+----+----+----+----+----+
   */
  _mm_store_si128(&((__m128i*)out)[3], _mm_unpackhi_epi64(p, q));

  /*
    +----+----+----+----+----+----+----+----+
    |  5 | 21 |  6 | 22 |  7 | 23 |  8 | 24 |
    +----+----+----+----+----+----+----+----+
   */
  a = _mm_unpackhi_epi16(row0, row2);
  /*
    +----+----+----+----+----+----+----+----+
    | 13 | 29 | 14 | 30 | 15 | 31 | 16 | 32 |
    +----+----+----+----+----+----+----+----+
   */
  b = _mm_unpackhi_epi16(row1, row3);
  /*
    +----+----+----+----+----+----+----+----+
    |  5 | 13 | 21 | 29 |  6 | 14 | 22 | 30 |
    +----+----+----+----+----+----+----+----+
   */
  p = _mm_unpacklo_epi16(a, b);

  /*
    +----+----+----+----+----+----+----+----+
    | 37 | 53 | 38 | 54 | 39 | 55 | 40 | 56 |
    +----+----+----+----+----+----+----+----+
   */
  c = _mm_unpackhi_epi16(row4, row6);
  /*
    +----+----+----+----+----+----+----+----+
    | 45 | 61 | 46 | 62 | 47 | 63 | 48 | 64 |
    +----+----+----+----+----+----+----+----+
   */
  d = _mm_unpackhi_epi16(row5, row7);
  /*
    +----+----+----+----+----+----+----+----+
    | 37 | 45 | 53 | 61 | 38 | 46 | 54 | 62 |
    +----+----+----+----+----+----+----+----+
   */
  q = _mm_unpacklo_epi16(c, d);

  _mm_store_si128(&((__m128i*)out)[4], _mm_unpacklo_epi64(p, q));
  _mm_store_si128(&((__m128i*)out)[5], _mm_unpackhi_epi64(p, q));

  /*
    +----+----+----+----+----+----+----+----+
    |  7 | 15 | 23 | 31 |  8 | 16 | 24 | 32 |
    +----+----+----+----+----+----+----+----+
   */
  p = _mm_unpackhi_epi16(a, b);
  /*
    +----+----+----+----+----+----+----+----+
    | 39 | 47 | 55 | 63 | 40 | 48 | 56 | 64 |
    +----+----+----+----+----+----+----+----+
   */
  q = _mm_unpackhi_epi16(c, d);

  _mm_store_si128(&((__m128i*)out)[6], _mm_unpacklo_epi64(p, q));
  _mm_store_si128(&((__m128i*)out)[7], _mm_unpackhi_epi64(p, q));
}

/*
  assuming that a and b is aligned to 16 bytes (!!!!)

  we are using uint32_t because then we can fit eight of them
  in each 256-bit XMM register (AVX2).
*/
void transpose_8x8_int32(uint32_t src[8 * 8], uint32_t out[8 * 8]) {
  /* function provided by: http://stackoverflow.com/a/25627536 */
  __m256i* in = (__m256i*)src;

  __m256i row0, row1, row2, row3, row4, row5, row6, row7;

  row0 = in[0];
  row1 = in[1];
  row2 = in[2];
  row3 = in[3];
  row4 = in[4];
  row5 = in[5];
  row6 = in[6];
  row7 = in[7];

  __m256 __t0, __t1, __t2, __t3, __t4, __t5, __t6, __t7;
  __m256 __tt0, __tt1, __tt2, __tt3, __tt4, __tt5, __tt6, __tt7;

  /*
    +----+----+----+----+----+----+----+----+
    |  1 |  9 |  2 | 10 |  5 | 13 |  6 | 14 |
    +----+----+----+----+----+----+----+----+
   */
  __t0 = _mm256_unpacklo_epi32(row0, row1);
  /*
    +----+----+----+----+----+----+----+----+
    |  3 | 11 |  4 | 12 |  7 | 15 |  8 | 16 |
    +----+----+----+----+----+----+----+----+
   */
  __t1 = _mm256_unpackhi_epi32(row0, row1);
  /*
    +----+----+----+----+----+----+----+----+
    | 17 | 25 | 18 | 26 | 21 | 29 | 22 | 30 |
    +----+----+----+----+----+----+----+----+
   */
  __t2 = _mm256_unpacklo_epi32(row2, row3);
  /*
    +----+----+----+----+----+----+----+----+
    | 19 | 27 | 20 | 28 | 23 | 31 | 24 | 32 |
    +----+----+----+----+----+----+----+----+
   */
  __t3 = _mm256_unpackhi_epi32(row2, row3);
  /*
    +----+----+----+----+----+----+----+----+
    | 33 | 41 | 34 | 42 | 37 | 45 | 38 | 46 |
    +----+----+----+----+----+----+----+----+
   */
  __t4 = _mm256_unpacklo_epi32(row4, row5);
  /*
    +----+----+----+----+----+----+----+----+
    | 35 | 43 | 36 | 44 | 39 | 47 | 40 | 48 |
    +----+----+----+----+----+----+----+----+
   */
  __t5 = _mm256_unpackhi_epi32(row4, row5);
  /*
    +----+----+----+----+----+----+----+----+
    | 49 | 57 | 50 | 58 | 53 | 61 | 54 | 62 |
    +----+----+----+----+----+----+----+----+
   */
  __t6 = _mm256_unpacklo_epi32(row6, row7);
  /*
    +----+----+----+----+----+----+----+----+
    | 51 | 59 | 52 | 60 | 55 | 63 | 56 | 64 |
    +----+----+----+----+----+----+----+----+
   */
  __t7 = _mm256_unpackhi_epi32(row6, row7);

  /*
    __t0:
    +----+----+----+----+----+----+----+----+
    |  1 |  9 |  2 | 10 |  5 | 13 |  6 | 14 |
    +----+----+----+----+----+----+----+----+

    __t2:
    +----+----+----+----+----+----+----+----+
    | 17 | 25 | 18 | 26 | 21 | 29 | 22 | 30 |
    +----+----+----+----+----+----+----+----+

    _MM_SHUFFLE(1,0,1,0):
    lower 128-bits:
    - dst[0] = __t0[0]
    - dst[1] = __t0[1]
    - dst[2] = __t2[0]
    - dst[3] = __t2[1]
    upper 128-bits:
    - dst[4] = __t0[4]
    - dst[5] = __t0[5]
    - dst[6] = __t2[4]
    - dst[7] = __t2[5]

    __tt0:
    +----+----+----+----+----+----+----+----+
    |  1 |  9 | 17 | 25 |  5 | 13 | 21 | 29 |
    +----+----+----+----+----+----+----+----+
   */
  // in AVX-512, we can use: _mm256_shuffle_i32x4()
  __tt0 = _mm256_shuffle_ps(__t0,__t2,_MM_SHUFFLE(1,0,1,0));
  /*
    _MM_SHUFFLE(z, y, x, w) = (z << 6) | (y << 4) | (x << 2) | w:
    z and y selects from __t2
    x and w selects from __t1

    w and y selects first element,
    x and z selects second element

    _MM_SHUFFLE(3, 2, 3, 2) = (3 << 6) | (2 << 4) | (3 << 2) | 2:
    lower 128-bits:
    - dst[0] = __t0[2] (w)
    - dst[1] = __t0[3] (x)
    - dst[2] = __t2[2] (y)
    - dst[3] = __t2[3] (z)
    upper 128-bits:
    - dst[4] = __t0[6] (w)
    - dst[5] = __t0[7] (x)
    - dst[6] = __t2[6] (y)
    - dst[7] = __t2[7] (z)

    __tt1:
    +----+----+----+----+----+----+----+----+
    |  2 | 10 | 18 | 26 |  6 | 14 | 22 | 30 |
    +----+----+----+----+----+----+----+----+
   */
  __tt1 = _mm256_shuffle_ps(__t0,__t2,_MM_SHUFFLE(3,2,3,2));
  /*
    +----+----+----+----+----+----+----+----+
    |  3 | 11 | 19 | 27 |  7 | 15 | 23 | 31 |
    +----+----+----+----+----+----+----+----+
   */
  __tt2 = _mm256_shuffle_ps(__t1,__t3,_MM_SHUFFLE(1,0,1,0));
  /*
    +----+----+----+----+----+----+----+----+
    |  4 | 12 | 20 | 28 |  8 | 16 | 24 | 32 |
    +----+----+----+----+----+----+----+----+
   */
  __tt3 = _mm256_shuffle_ps(__t1,__t3,_MM_SHUFFLE(3,2,3,2));
  /*
    +----+----+----+----+----+----+----+----+
    | 33 | 41 | 49 | 57 | 37 | 45 | 53 | 61 |
    +----+----+----+----+----+----+----+----+
   */
  __tt4 = _mm256_shuffle_ps(__t4,__t6,_MM_SHUFFLE(1,0,1,0));
  /*
    +----+----+----+----+----+----+----+----+
    | 34 | 42 | 50 | 58 | 38 | 46 | 54 | 62 |
    +----+----+----+----+----+----+----+----+
   */
  __tt5 = _mm256_shuffle_ps(__t4,__t6,_MM_SHUFFLE(3,2,3,2));
  /*
    +----+----+----+----+----+----+----+----+
    | 35 | 43 | 51 | 59 | 39 | 47 | 55 | 63 |
    +----+----+----+----+----+----+----+----+
   */
  __tt6 = _mm256_shuffle_ps(__t5,__t7,_MM_SHUFFLE(1,0,1,0));
  /*
    +----+----+----+----+----+----+----+----+
    | 36 | 44 | 52 | 60 | 40 | 48 | 56 | 64 |
    +----+----+----+----+----+----+----+----+
   */
  __tt7 = _mm256_shuffle_ps(__t5,__t7,_MM_SHUFFLE(3,2,3,2));

  _mm256_store_si256(&((__m256i*)out)[0], _mm256_permute2f128_ps(__tt0, __tt4, 0x20));
  _mm256_store_si256(&((__m256i*)out)[1], _mm256_permute2f128_ps(__tt1, __tt5, 0x20));
  _mm256_store_si256(&((__m256i*)out)[2], _mm256_permute2f128_ps(__tt2, __tt6, 0x20));
  _mm256_store_si256(&((__m256i*)out)[3], _mm256_permute2f128_ps(__tt3, __tt7, 0x20));
  _mm256_store_si256(&((__m256i*)out)[4], _mm256_permute2f128_ps(__tt0, __tt4, 0x31));
  _mm256_store_si256(&((__m256i*)out)[5], _mm256_permute2f128_ps(__tt1, __tt5, 0x31));
  _mm256_store_si256(&((__m256i*)out)[6], _mm256_permute2f128_ps(__tt2, __tt6, 0x31));
  _mm256_store_si256(&((__m256i*)out)[7], _mm256_permute2f128_ps(__tt3, __tt7, 0x31));
}

void transpose_8x8_int32_slower(uint32_t src[8 * 8], uint32_t out[8 * 8]) {
  /*
    consider input:

    row +----+----+----+----+----+----+----+----+
     0  |  1 |  2 |  3 |  4 |  5 |  6 |  7 |  8 |
        +----+----+----+----+----+----+----+----+
     1  |  9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 |
        +----+----+----+----+----+----+----+----+
     2  | 17 | 18 | 19 | 20 | 21 | 22 | 23 | 24 |
        +----+----+----+----+----+----+----+----+
     3  | 25 | 26 | 27 | 28 | 29 | 30 | 31 | 32 |
        +----+----+----+----+----+----+----+----+
     4  | 33 | 34 | 35 | 36 | 37 | 38 | 39 | 40 |
        +----+----+----+----+----+----+----+----+
     5  | 41 | 42 | 43 | 44 | 45 | 46 | 47 | 48 |
        +----+----+----+----+----+----+----+----+
     6  | 49 | 50 | 51 | 52 | 53 | 54 | 55 | 56 |
        +----+----+----+----+----+----+----+----+
     7  | 57 | 58 | 59 | 60 | 61 | 62 | 63 | 64 |
        +----+----+----+----+----+----+----+----+
   */
  __m256i* in = (__m256i*)src;

  __m256i row0, row1, row2, row3, row4, row5, row6, row7;

  row0 = in[0];
  row1 = in[1];
  row2 = in[2];
  row3 = in[3];
  row4 = in[4];
  row5 = in[5];
  row6 = in[6];
  row7 = in[7];

  __m256i a, b, c, d;
  __m256i p, q;

  /*
    a = _mm256_permute2f128_si256(row0, row1, 0b00000000):
    +----+----+----+----+----+----+----+----+
    |  1 |  2 |  3 |  4 |  1 |  2 |  3 |  4 |
    +----+----+----+----+----+----+----+----+

    a = _mm256_permute2f128_si256(row0, row1, 0b00010000):
    +----+----+----+----+----+----+----+----+
    |  1 |  2 |  3 |  4 |  5 |  6 |  7 |  8 |
    +----+----+----+----+----+----+----+----+
   */

  /*
    get the first four 32-bit integers ...
    +----+----+----+----+----+----+----+----+
    |  1 | 17 |  2 | 18 |  0 |  0 |  0 |  0 |
    +----+----+----+----+----+----+----+----+
   */
  a = _mm256_castsi128_si256(_mm_unpacklo_epi32(*((__m128i*)&row0), *((__m128i*)&row2)));

  /*
    get the next four ...
    +----+----+----+----+----+----+----+----+
    |  3 | 19 |  4 | 20 |  0 |  0 |  0 |  0 |
    +----+----+----+----+----+----+----+----+

    and copy the upper 128-bits into the lower 128-bits of a:
    +----+----+----+----+----+----+----+----+
    |  1 | 17 |  2 | 18 |  3 | 19 |  4 | 20 |
    +----+----+----+----+----+----+----+----+
   */
  a = _mm256_inserti128_si256(a, _mm_unpackhi_epi32(*((__m128i*)&row0), *((__m128i*)&row2)), 1);

  /*
    +----+----+----+----+----+----+----+----+
    |  9 | 25 | 10 | 26 | 11 | 27 | 12 | 28 |
    +----+----+----+----+----+----+----+----+
   */
  b = _mm256_castsi128_si256(_mm_unpacklo_epi32(*((__m128i*)&row1), *((__m128i*)&row3)));
  b = _mm256_inserti128_si256(b, _mm_unpackhi_epi32(*((__m128i*)&row1), *((__m128i*)&row3)), 1);
  /*
    +----+----+----+----+----+----+----+----+
    |  1 |  9 | 17 | 25 |  2 | 10 | 18 | 26 |
    +----+----+----+----+----+----+----+----+
   */
  p = _mm256_castsi128_si256(_mm_unpacklo_epi32(*((__m128i*)&a), *((__m128i*)&b)));
  p = _mm256_inserti128_si256(p, _mm_unpackhi_epi32(*((__m128i*)&a), *((__m128i*)&b)), 1);

  /*
    +----+----+----+----+----+----+----+----+
    | 33 | 49 | 34 | 50 | 35 | 51 | 36 | 52 |
    +----+----+----+----+----+----+----+----+
   */
  c = _mm256_castsi128_si256(_mm_unpacklo_epi32(*((__m128i*)&row4), *((__m128i*)&row6)));
  c = _mm256_inserti128_si256(c, _mm_unpackhi_epi32(*((__m128i*)&row4), *((__m128i*)&row6)), 1);
  /*
    +----+----+----+----+----+----+----+----+
    | 41 | 57 | 42 | 58 | 43 | 59 | 44 | 60 |
    +----+----+----+----+----+----+----+----+
   */
  d = _mm256_castsi128_si256(_mm_unpacklo_epi32(*((__m128i*)&row5), *((__m128i*)&row7)));
  d = _mm256_inserti128_si256(d, _mm_unpackhi_epi32(*((__m128i*)&row5), *((__m128i*)&row7)), 1);
  /*
    +----+----+----+----+----+----+----+----+
    | 33 | 41 | 49 | 57 | 34 | 42 | 50 | 58 |
    +----+----+----+----+----+----+----+----+
   */
  q = _mm256_castsi128_si256(_mm_unpacklo_epi32(*((__m128i*)&c), *((__m128i*)&d)));
  q = _mm256_inserti128_si256(q, _mm_unpackhi_epi32(*((__m128i*)&c), *((__m128i*)&d)), 1);

  /*
    first 128-bit half of p + first 128-bit half of q
    +----+----+----+----+----+----+----+----+
    |  1 |  9 | 17 | 25 | 33 | 41 | 49 | 57 |
    +----+----+----+----+----+----+----+----+

    would be equal to:
    a = _mm256_permute2f128_si256(p, q, 0x20)
   */
  _mm256_store_si256(&((__m256i*)out)[0], _mm256_inserti128_si256(p, *((__m128i*)&q), 1));
  /*
    last 128-bit half of p + last 128-bit half of q
    +----+----+----+----+----+----+----+----+
    |  2 | 10 | 18 | 26 | 34 | 42 | 50 | 58 |
    +----+----+----+----+----+----+----+----+
   */
  _mm256_store_si256(&((__m256i*)out)[1], _mm256_inserti128_si256(q, ((__m128i*)&p)[1], 0));

  /*
    +----+----+----+----+----+----+----+----+
    |  3 | 11 | 19 | 27 |  4 | 12 | 20 | 28 |
    +----+----+----+----+----+----+----+----+
   */
  p = _mm256_castsi128_si256(_mm_unpacklo_epi32(((__m128i*)&a)[1], ((__m128i*)&b)[1]));
  p = _mm256_inserti128_si256(p, _mm_unpackhi_epi32(((__m128i*)&a)[1], ((__m128i*)&b)[1]), 1);

  /*
    +----+----+----+----+----+----+----+----+
    | 35 | 43 | 51 | 59 | 36 | 44 | 52 | 60 |
    +----+----+----+----+----+----+----+----+
   */
  q = _mm256_castsi128_si256(_mm_unpacklo_epi32(((__m128i*)&c)[1], ((__m128i*)&d)[1]));
  q = _mm256_inserti128_si256(q, _mm_unpackhi_epi32(((__m128i*)&c)[1], ((__m128i*)&d)[1]), 1);

  /*
    +----+----+----+----+----+----+----+----+
    |  3 | 11 | 19 | 27 | 35 | 43 | 51 | 59 |
    +----+----+----+----+----+----+----+----+
   */
  _mm256_store_si256(&((__m256i*)out)[2], _mm256_inserti128_si256(p, *((__m128i*)&q), 1));

  /*
    +----+----+----+----+----+----+----+----+
    |  4 | 12 | 20 | 28 | 36 | 44 | 52 | 60 |
    +----+----+----+----+----+----+----+----+
   */
  _mm256_store_si256(&((__m256i*)out)[3], _mm256_inserti128_si256(q, ((__m128i*)&p)[1], 0));

  /*
    +----+----+----+----+----+----+----+----+
    |  5 | 21 |  6 | 22 |  7 | 23 |  8 | 24 |
    +----+----+----+----+----+----+----+----+
   */
  a = _mm256_castsi128_si256(_mm_unpacklo_epi32(((__m128i*)&row0)[1], ((__m128i*)&row2)[1]));
  a = _mm256_inserti128_si256(a, _mm_unpackhi_epi32(((__m128i*)&row0)[1], ((__m128i*)&row2)[1]), 1);

  /*
    +----+----+----+----+----+----+----+----+
    | 13 | 29 | 14 | 30 | 15 | 31 | 16 | 32 |
    +----+----+----+----+----+----+----+----+
   */
  b = _mm256_castsi128_si256(_mm_unpacklo_epi32(((__m128i*)&row1)[1], ((__m128i*)&row3)[1]));
  b = _mm256_inserti128_si256(b, _mm_unpackhi_epi32(((__m128i*)&row1)[1], ((__m128i*)&row3)[1]), 1);
  /*
    +----+----+----+----+----+----+----+----+
    |  5 | 13 | 21 | 29 |  6 | 14 | 22 | 30 |
    +----+----+----+----+----+----+----+----+
   */
  p = _mm256_castsi128_si256(_mm_unpacklo_epi32(*((__m128i*)&a), *((__m128i*)&b)));
  p = _mm256_inserti128_si256(p, _mm_unpackhi_epi32(*((__m128i*)&a), *((__m128i*)&b)), 1);

  /*
    +----+----+----+----+----+----+----+----+
    | 37 | 53 | 38 | 54 | 39 | 55 | 40 | 56 |
    +----+----+----+----+----+----+----+----+
   */
  c = _mm256_castsi128_si256(_mm_unpacklo_epi32(((__m128i*)&row4)[1], ((__m128i*)&row6)[1]));
  c = _mm256_inserti128_si256(c, _mm_unpackhi_epi32(((__m128i*)&row4)[1], ((__m128i*)&row6)[1]), 1);

  /*
    +----+----+----+----+----+----+----+----+
    | 45 | 61 | 46 | 62 | 47 | 63 | 48 | 64 |
    +----+----+----+----+----+----+----+----+
   */
  d = _mm256_castsi128_si256(_mm_unpacklo_epi32(((__m128i*)&row5)[1], ((__m128i*)&row7)[1]));
  d = _mm256_inserti128_si256(d, _mm_unpackhi_epi32(((__m128i*)&row5)[1], ((__m128i*)&row7)[1]), 1);

  /*
    +----+----+----+----+----+----+----+----+
    | 37 | 45 | 53 | 61 | 38 | 46 | 54 | 62 |
    +----+----+----+----+----+----+----+----+
   */
  q = _mm256_castsi128_si256(_mm_unpacklo_epi32(*((__m128i*)&c), *((__m128i*)&d)));
  q = _mm256_inserti128_si256(q, _mm_unpackhi_epi32(*((__m128i*)&c), *((__m128i*)&d)), 1);

  /*
    +----+----+----+----+----+----+----+----+
    |  5 | 13 | 21 | 29 | 37 | 45 | 53 | 61 |
    +----+----+----+----+----+----+----+----+
   */
  _mm256_store_si256(&((__m256i*)out)[4], _mm256_inserti128_si256(p, *((__m128i*)&q), 1));
  /*
    +----+----+----+----+----+----+----+----+
    |  6 | 14 | 22 | 30 | 38 | 46 | 54 | 62 |
    +----+----+----+----+----+----+----+----+
   */
  _mm256_store_si256(&((__m256i*)out)[5], _mm256_inserti128_si256(q, ((__m128i*)&p)[1], 0));

  /*
    +----+----+----+----+----+----+----+----+
    |  7 | 15 | 23 | 31 |  8 | 16 | 24 | 32 |
    +----+----+----+----+----+----+----+----+
   */
  p = _mm256_castsi128_si256(_mm_unpacklo_epi32(((__m128i*)&a)[1], ((__m128i*)&b)[1]));
  p = _mm256_inserti128_si256(p, _mm_unpackhi_epi32(((__m128i*)&a)[1], ((__m128i*)&b)[1]), 1);

  /*
    +----+----+----+----+----+----+----+----+
    | 39 | 47 | 55 | 63 | 40 | 48 | 56 | 64 |
    +----+----+----+----+----+----+----+----+
   */
  q = _mm256_castsi128_si256(_mm_unpacklo_epi32(((__m128i*)&c)[1], ((__m128i*)&d)[1]));
  q = _mm256_inserti128_si256(q, _mm_unpackhi_epi32(((__m128i*)&c)[1], ((__m128i*)&d)[1]), 1);

  _mm256_store_si256(&((__m256i*)out)[6], _mm256_inserti128_si256(p, *((__m128i*)&q), 1));
  _mm256_store_si256(&((__m256i*)out)[7], _mm256_inserti128_si256(q, ((__m128i*)&p)[1], 0));
}
